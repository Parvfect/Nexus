{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "192e72f4",
   "metadata": {},
   "source": [
    "## Semantic scholar\n",
    "Switching to the api to see if it can give me more fulltexts in general"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "30a96d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from utils import read_api_key, extract_pdf_text_from_url\n",
    "from semantic_scholar_functions import search_for_paper, retrieve_paper, bulk_retrieve_papers, get_paper_pdf_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6e5275df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "06fe3dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_title = 'Deep learning via Hessian free optimization'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8919c117",
   "metadata": {},
   "outputs": [],
   "source": [
    "t = search_for_paper(paper_title=search_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bfa9a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_id = t['data'][1]['paperId']\n",
    "paper = retrieve_paper(paper_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "208e132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_ids = [i['paperId'] for i in paper['references']]\n",
    "reference_papers = bulk_retrieve_papers(reference_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "60b42bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [paper] + reference_papers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b11c8430",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = [i for i in papers if i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "78dc1ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Error in extracting pdf url for {'paperId': '836562bc282cfcfd6856de072a1204fc2e467b91', 'externalIds': {'MAG': '3102934511', 'DBLP': 'conf/nips/KimYK20', 'CorpusId': 226299742}, 'corpusId': 226299742, 'title': 'Position-based Scaled Gradient for Model Quantization and Pruning', 'year': 2020, 'referenceCount': 45, 'citationCount': 31, 'influentialCitationCount': 4, 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'fieldsOfStudy': ['Computer Science', 'Mathematics'], 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'}, {'category': 'Mathematics', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}], 'tldr': {'model': 'tldr@v2.0.0', 'text': 'The position-based scaled gradient (PSG) that scales the gradient depending on the position of a weight vector to make it more compression-friendly and reduces the gap between the weight distributions of a full-precision model and its compressed counterpart.'}, 'journal': {'name': 'arXiv: Computer Vision and Pattern Recognition', 'volume': ''}, 'authors': [{'authorId': '49476045', 'name': 'Jangho Kim'}, {'authorId': '1713608836', 'name': 'Kiyoon Yoo'}, {'authorId': '3160425', 'name': 'Nojun Kwak'}], 'references': None}\n",
      "ERROR:root:Error in extracting pdf url for {'paperId': 'f789425a7af1d012675118d7d10cd50afad09074', 'externalIds': {'DBLP': 'conf/nips/BannerNS19', 'MAG': '2970601456', 'CorpusId': 59292009}, 'corpusId': 59292009, 'title': 'Post training 4-bit quantization of convolutional networks for rapid-deployment', 'year': 2018, 'referenceCount': 28, 'citationCount': 600, 'influentialCitationCount': 58, 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'fieldsOfStudy': ['Computer Science'], 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}], 'tldr': {'model': 'tldr@v2.0.0', 'text': 'This paper introduces the first practical 4-bit post training quantization approach: it does not involve training the quantized model (fine-tuning), nor it requires the availability of the full dataset, and achieves accuracy that is just a few percents less the state-of-the-art baseline across a wide range of convolutional models.'}, 'journal': {'pages': '7948-7956'}, 'authors': [{'authorId': '2607278', 'name': 'Ron Banner'}, {'authorId': '51498210', 'name': 'Yury Nahshan'}, {'authorId': '1912398', 'name': 'Daniel Soudry'}], 'references': None}\n",
      "ERROR:root:Error in extracting pdf url for {'paperId': 'a42954d4b9d0ccdf1036e0af46d87a01b94c3516', 'externalIds': {'DBLP': 'conf/nips/HassibiS92', 'MAG': '2125389748', 'CorpusId': 7057040}, 'corpusId': 7057040, 'title': 'Second Order Derivatives for Network Pruning: Optimal Brain Surgeon', 'year': 1992, 'referenceCount': 12, 'citationCount': 1993, 'influentialCitationCount': 95, 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'fieldsOfStudy': ['Computer Science'], 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}], 'tldr': {'model': 'tldr@v2.0.0', 'text': 'Of OBS, Optimal Brain Damage, and magnitude-based methods, only OBS deletes the correct weights from a trained XOR network in every case, and thus yields better generalization on test data.'}, 'journal': {'pages': '164-171'}, 'authors': [{'authorId': '1736279', 'name': 'B. Hassibi'}, {'authorId': '2586918', 'name': 'D. Stork'}], 'references': None}\n",
      "ERROR:root:Error in extracting pdf url for {'paperId': 'aaab42855e103af9052173e24506bbbc31d68421', 'externalIds': {'DBLP': 'conf/nips/WangZLL22', 'CorpusId': 258509009}, 'corpusId': 258509009, 'title': 'Leveraging Inter-Layer Dependency for Post -Training Quantization', 'year': 2022, 'referenceCount': 51, 'citationCount': 20, 'influentialCitationCount': 5, 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'fieldsOfStudy': ['Computer Science'], 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}], 'tldr': {'model': 'tldr@v2.0.0', 'text': 'A novel Network-Wise Quantization approach to fully leveraging inter-layer dependency and faces a larger scale combinatorial optimization problem of discrete variables than in previous works, which raises two major challenges: over-fitting and discrete optimization problem.'}, 'journal': None, 'authors': [{'authorId': '2146410921', 'name': 'Changbao Wang'}, {'authorId': '2216487433', 'name': 'Dandan Zheng'}, {'authorId': '2816557', 'name': 'Yuanliu Liu'}, {'authorId': '2181041513', 'name': 'Liang Li'}], 'references': None}\n",
      "ERROR:root:Error in extracting pdf url for {'paperId': 'd2a2677f6594a316101176c607ad6eae094bba44', 'externalIds': {'CorpusId': 265038491}, 'corpusId': 265038491, 'title': 'HAWQ-V3: Dyadic Neural Network Quantization', 'year': 2021, 'referenceCount': 0, 'citationCount': 176, 'influentialCitationCount': 25, 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'fieldsOfStudy': None, 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 's2-fos-model'}], 'tldr': {'model': 'tldr@v2.0.0', 'text': 'This analysis adopted to use TVM (Chen et al., 2018), which provides a general graph and a tensor expression intermediate representation (IR) to support automatic code transformation and generation and the decoupled IR design in TVM allows the mixed-precision quantization optimization to be applied without affecting the specification of algorithms.'}, 'journal': None, 'authors': [], 'references': None}\n",
      "ERROR:root:Error in extracting pdf url for {'paperId': '851c2e1942642537499c743c324f10624b7b77ac', 'externalIds': {'DBLP': 'conf/icml/HubaraNHBS21', 'CorpusId': 235825979}, 'corpusId': 235825979, 'title': 'Accurate Post Training Quantization With Small Calibration Sets', 'year': 2021, 'referenceCount': 28, 'citationCount': 184, 'influentialCitationCount': 21, 'openAccessPdf': {'url': '', 'status': None, 'license': None}, 'fieldsOfStudy': ['Computer Science'], 's2FieldsOfStudy': [{'category': 'Computer Science', 'source': 'external'}, {'category': 'Computer Science', 'source': 's2-fos-model'}], 'tldr': {'model': 'tldr@v2.0.0', 'text': 'This work empirically demonstrates that this approach is much less susceptible to over-ﬁtting than the standard ﬁne-tuning approaches, and can be used even on a very small calibration set; and more powerful than previous methods, which only set the activations’ dynamic ranges.'}, 'journal': {'pages': '4466-4475'}, 'authors': [{'authorId': '2477463', 'name': 'Itay Hubara'}, {'authorId': '51498210', 'name': 'Yury Nahshan'}, {'authorId': '48967743', 'name': 'Yair Hanani'}, {'authorId': '2607278', 'name': 'Ron Banner'}, {'authorId': '1912398', 'name': 'Daniel Soudry'}], 'references': None}\n",
      "ERROR:root:Error in extracting pdf url for None\n"
     ]
    }
   ],
   "source": [
    "paper_urls = get_paper_pdf_urls(reference_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "86e7107c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(paper_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "2c067c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_fulltexts = [extract_pdf_text_from_url(i) if i else '' for i in paper_urls]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa8063f",
   "metadata": {},
   "source": [
    "### Make graph nodes and graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "67887ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_members import SemanticNode, Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20703434",
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = [SemanticNode(i) for i in papers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "58cdbe8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = Graph(nodes = nodes, primary_node=nodes[0], search_query=search_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "5d2e9b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, j in zip(graph.nodes, paper_fulltexts):\n",
    "    if j:\n",
    "        i.has_fulltext = True\n",
    "    else:\n",
    "        i.has_fulltext = False\n",
    "        \n",
    "    i.fulltext = j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b586b1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64781a07",
   "metadata": {},
   "source": [
    "### Relevance search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c8cb0105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56545354929341d7a4db7fdfd5d00fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b105990005a4487c8abe4096e2559728",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.10.0.json:   0%|  …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from paper_similarity import get_paper_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "57496210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8867924528301887 % completed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:438: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "c:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8867924528301887 % completed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8867924528301887 % completed\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n",
      "c:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1541: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
      "  return forward_call(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.886792452830189 % completed\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[100], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(node, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfulltext\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m----> 8\u001b[0m         relevance, k1, k2 \u001b[38;5;241m=\u001b[39m \u001b[43mget_paper_similarity\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprimary_node\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfulltext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfulltext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mException \u001b[39m\u001b[38;5;132;01m{e}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Parv\\Doc\\Nexus\\paper_similarity.py:87\u001b[0m, in \u001b[0;36mget_paper_similarity\u001b[1;34m(paper1_text, paper2_text)\u001b[0m\n\u001b[0;32m     84\u001b[0m paper2_text \u001b[38;5;241m=\u001b[39m clean_text(expand_and_remove_acronyms(fix_split_words(paper2_text)))\n\u001b[0;32m     86\u001b[0m \u001b[38;5;66;03m# Extract keywords (always guaranteed)\u001b[39;00m\n\u001b[1;32m---> 87\u001b[0m keywords1 \u001b[38;5;241m=\u001b[39m \u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpaper1_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     88\u001b[0m keywords2 \u001b[38;5;241m=\u001b[39m extract_keywords(paper2_text)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Compute similarity\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Parv\\Doc\\Nexus\\paper_similarity.py:49\u001b[0m, in \u001b[0;36mextract_keywords\u001b[1;34m(text, top_n)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_keywords\u001b[39m(text, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m):\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;66;03m# Primary: KeyBERT\u001b[39;00m\n\u001b[1;32m---> 49\u001b[0m     keywords \u001b[38;5;241m=\u001b[39m \u001b[43mkw_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_keywords\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     50\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     51\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeyphrase_ngram_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     52\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43menglish\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     53\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_maxsum\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtop_n\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     keywords \u001b[38;5;241m=\u001b[39m [kw \u001b[38;5;28;01mfor\u001b[39;00m kw, _ \u001b[38;5;129;01min\u001b[39;00m keywords]\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;66;03m# Fallback: Noun extraction if KeyBERT fails\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\keybert\\_model.py:235\u001b[0m, in \u001b[0;36mKeyBERT.extract_keywords\u001b[1;34m(self, docs, candidates, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer, highlight, seed_keywords, doc_embeddings, word_embeddings, threshold)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;66;03m# Max Sum Distance\u001b[39;00m\n\u001b[0;32m    234\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m use_maxsum:\n\u001b[1;32m--> 235\u001b[0m     keywords \u001b[38;5;241m=\u001b[39m \u001b[43mmax_sum_distance\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdoc_embedding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    237\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    238\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    239\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtop_n\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    240\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnr_candidates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Cosine-based keyword extraction\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    245\u001b[0m     distances \u001b[38;5;241m=\u001b[39m cosine_similarity(doc_embedding, candidate_embeddings)\n",
      "File \u001b[1;32mc:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\keybert\\_maxsum.py:40\u001b[0m, in \u001b[0;36mmax_sum_distance\u001b[1;34m(doc_embedding, word_embeddings, words, top_n, nr_candidates)\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Calculate distances and extract keywords\u001b[39;00m\n\u001b[0;32m     39\u001b[0m distances \u001b[38;5;241m=\u001b[39m cosine_similarity(doc_embedding, word_embeddings)\n\u001b[1;32m---> 40\u001b[0m distances_words \u001b[38;5;241m=\u001b[39m \u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Get 2*top_n words as candidates based on cosine similarity\u001b[39;00m\n\u001b[0;32m     43\u001b[0m words_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(distances\u001b[38;5;241m.\u001b[39margsort()[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39mnr_candidates:])\n",
      "File \u001b[1;32mc:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\sklearn\\metrics\\pairwise.py:1676\u001b[0m, in \u001b[0;36mcosine_similarity\u001b[1;34m(X, Y, dense_output)\u001b[0m\n\u001b[0;32m   1673\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1674\u001b[0m     Y_normalized \u001b[38;5;241m=\u001b[39m normalize(Y, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 1676\u001b[0m K \u001b[38;5;241m=\u001b[39m \u001b[43msafe_sparse_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_normalized\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_normalized\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdense_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdense_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m K\n",
      "File \u001b[1;32mc:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\sklearn\\utils\\extmath.py:211\u001b[0m, in \u001b[0;36msafe_sparse_dot\u001b[1;34m(a, b, dense_output)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     ret \u001b[38;5;241m=\u001b[39m a \u001b[38;5;241m@\u001b[39m b\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m--> 211\u001b[0m     \u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    212\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m sparse\u001b[38;5;241m.\u001b[39missparse(b)\n\u001b[0;32m    213\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m dense_output\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(ret, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoarray\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    215\u001b[0m ):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39mtoarray()\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\Parv\\anaconda3\\envs\\pytorch_gpu\\Lib\\site-packages\\scipy\\sparse\\_base.py:1513\u001b[0m, in \u001b[0;36missparse\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1508\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m sparray\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m \u001b[38;5;241m=\u001b[39m _spbase\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__doc__\u001b[39m\n\u001b[1;32m-> 1513\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21missparse\u001b[39m(x):\n\u001b[0;32m   1514\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Is `x` of a sparse array or sparse matrix type?\u001b[39;00m\n\u001b[0;32m   1515\u001b[0m \n\u001b[0;32m   1516\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1539\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, _spbase)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i, node in enumerate(graph.nodes[1:]):\n",
    "\n",
    "    progress_bar = i+1 / (len(graph.nodes) - 1) * 100\n",
    "    print(f\"{progress_bar} % completed\\n\")\n",
    "\n",
    "    try:\n",
    "        if hasattr(node, \"fulltext\"):\n",
    "            relevance, k1, k2 = get_paper_similarity(\n",
    "                graph.primary_node.fulltext, node.fulltext)\n",
    "    except Exception as e:\n",
    "        print(\"Exception {e}\")\n",
    "        node.relevance = 0.2\n",
    "        continue \n",
    "\n",
    "    node.relevance = float(relevance)\n",
    "    if k2:\n",
    "        node.keywords = k2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
